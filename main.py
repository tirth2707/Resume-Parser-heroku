# -*- coding: utf-8 -*-
"""Copy of resume_parser_complete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k496qVh6e3lUYqQmUnNsgEkzOtbj_cLu
"""

!pip install streamlit -q
!pip install PyPDF2
!pip install pyngrok

!pip install opencv-python-headless==4.1.2.30
!pip install easyocr
!apt-get install poppler-utils
!pip install pdf2image
!pip install phonenumbers
!pip install pdfminer.six
!pip install -U pip setuptools wheel
!pip install -U spacy
!python -m spacy download en_core_web_trf
!pip install Cython
!python -m spacy download en_core_web_sm
!python -m spacy download en_core_web_lg

import re
import io
import PIL
import json
import spacy
import easyocr
import numpy as np
import phonenumbers
import en_core_web_lg
from PIL import ImageDraw
from spacy import displacy
from spacy.tokens import Span
from google.colab import files
from spacy.lang.en import English
from spacy.matcher import Matcher
from pdf2image import convert_from_path
from IPython.display import display, Image
from phonenumbers.phonenumberutil import (
    region_code_for_country_code,
    region_code_for_number,
)
from pdfminer.high_level import extract_text
from spacy.matcher import PhraseMatcher
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize, sent_tokenize
from datetime import datetime
from dateutil import relativedelta
from spacy.lang.en import English
from spacy.pipeline import EntityRuler

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import streamlit.components.v1 as stc
# import os
# import pandas as pd
# import re
# import io
# import PIL
# import json
# import spacy
# import easyocr
# import numpy as np
# import phonenumbers
# import en_core_web_lg
# from PIL import ImageDraw
# from spacy import displacy
# from spacy.tokens import Span
# from google.colab import files
# from spacy.lang.en import English
# from spacy.matcher import Matcher
# from pdf2image import convert_from_path
# from IPython.display import display, Image
# from phonenumbers.phonenumberutil import (
#     region_code_for_country_code,
#     region_code_for_number,
# )
# from pdfminer.high_level import extract_text
# from spacy.matcher import PhraseMatcher
# import nltk
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('averaged_perceptron_tagger')
# from nltk.corpus import stopwords 
# from nltk.tokenize import word_tokenize, sent_tokenize
# from datetime import datetime
# from dateutil import relativedelta
# from spacy.lang.en import English
# from spacy.pipeline import EntityRuler
# 
# from PIL import Image 
# from PyPDF2 import PdfFileReader
# 
# header= st.container()
# fileupload= st.container()
# result=st.container()
# class ResumeParser:
# 
#     def __init__(self,file):
#         self.file=file
#         reader = easyocr.Reader(['en'])
#         self.images=convert_from_path(file)
#         self.bounds=[]
#         for i in self.images:
#             self.bounds.append(reader.readtext(np.array(i), min_size=0, slope_ths=0.2, ycenter_ths=0.7, height_ths=0.6,width_ths=0.8,decoder='beamsearch',beamWidth=10))
#         self.text = ''
#         for i in range(len(self.bounds)):
#             for j in range(len(self.bounds[i])):
#                 if self.bounds[i][j][1]!=None:
#                     self.text=self.text+self.bounds[i][j][1]+'\n'
#         skill_file = open("skills.txt", "r")
#         skill_content = skill_file.read()
#         self.skills = skill_content.split("\n")
#         self.trans_nlp=spacy.load("en_core_web_trf")
# 
# 
#     def get_text(self):
#         return self.text
# 
# 
#     def extract_mobile_number(self):
#         PHONE_REG = re.compile(r'[\+\(]?[1-9][0-9 .\-\(\)]{8,}[0-9]')
#         phones = re.findall(PHONE_REG, self.text)
#         num=[]
#         for phone in phones:
#             if phone:
#                 number = ''.join(phone)
#                 if self.text.find(number) >= 0 and len(number) < 16:
#                     num.append(number)
#         num_dir=[]
#         for i in num:
#             try:
#                 pn = phonenumbers.parse(i)
#                 num_dir.append([i,region_code_for_country_code(pn.country_code)])
#             except:
#                 num_dir.append([i,None])
#         return num_dir
#     
#     
#     def extract_gpa(self):
#         gpa_Regex=r'(\bgpa[ :]+)?(\d+(?:\.\d+)?)[/\d. ]{0,6}(?(1)| *gpa\b)'
#         gpa = re.findall(gpa_Regex, self.text)
#         return gpa
# 
# 
#     def extract_skills(self):
#         nlp=self.trans_nlp
#         phrase_matcher = PhraseMatcher(nlp.vocab)
#         skill_list=[nlp.make_doc(text) for text in self.skills]
#         phrase_matcher.add("SKILLS",None,*skill_list)
#         doc = nlp(self.get_text())
#         matches = phrase_matcher(doc)
#         unique_skills = []
#         for match_id, start, end in matches:
#             if doc[start:end].text not in unique_skills:
#                 unique_skills.append(doc[start:end].text)
#         return list(set(unique_skills))
# 
# 
#     def extract_languages(self):
#         phrase_matcher=PhraseMatcher(self.trans_nlp.vocab)
#         languages=[self.trans_nlp.make_doc(text) for text in ['Mandarin','Spanish','English','Hindi','Arabic','Portuguese','Russian','Japanese','Punjabi','German',
#                                                 'Javanese','Malay','Indonesian','Telugu','Vietnamese','Korean','French','Marathi','Tamil',
#                                                 'Urdu','Turkish','Italian','Yue (Cantonese)','Thai','Gujarati','Jin','Southern Min','Persian','Polish','Pashto','Kannada',
#                                                 'Xiang','Malayalam','Sundanese','Hausa', 'Odia','Oriya', 'Burmese','Hakka','Ukrainian','Bhojpuri','Tagalog','Yoruba',
#                                                 'Maithili','Uzbek','Uzbek','Sindhi','Amharic','Fula','Romanian','Oromo','Igbo','Azerbaijani','Awadhi','Gan Chinese','Cebuano (Visayan)',
#                                                 'Dutch','Kurdish','SerboCroatian','Malagasy','Saraiki','Nepali','Sinhalese','Chittagonian','Zhuang','Khmer','Turkmen','Assamese','Madurese',
#                                                 'Somali','Marwari','Magahi','Haryanvi','Hungarian','Chhattisgarhi','Greek','Chewa','Deccan','Akan','Kazakh','Northern Min','Sylheti','Zulu','Czech',
#                                                 'Kinyarwanda','Dhundhari','Haitian Creole','Eastern Min','Ilocano','Quechua','Kirundi','Swedish','Hmong','Shona','Uyghur','Hiligaynon','Mossi','Xhosa',
#                                                 'Belarusian','Balochi','Konkani','mandarin', 'spanish', 'english', 'hindi', 'arabic', 'portuguese', 'chinese', 'russian', 'japanese', 'punjabi', 'german',
#                                                 'javanese', 'wu (inc. shanghainese)', 'malay/indonesian', 'telugu', 'vietnamese', 'korean', 'french', 'marathi', 'tamil', 'urdu', 'turkish', 'italian', 
#                                                 'yue (cantonese)', 'thai', 'gujarati', 'jin', 'southern min', 'persian', 'polish', 'pashto', 'kannada', 'malayalam', 'sundanese', 
#                                                 'hausa', 'odia (oriya)', 'burmese', 'hakka', 'ukrainian', 'bhojpuri', 'tagalog', 'yoruba', 'maithili', 'uzbek', 'uzbek', 'sindhi', 'amharic', 'fula', 
#                                                 'romanian', 'oromo', 'igbo', 'azerbaijani', 'awadhi', 'gan chinese', 'cebuano (visayan)', 'dutch', 'kurdish', 'serbocroatian', 'malagasy', 'saraiki', 'nepali', 
#                                                 'sinhalese', 'chittagonian', 'zhuang', 'khmer', 'turkmen', 'assamese', 'madurese', 'somali', 'marwari', 'magahi', 'haryanvi', 'hungarian', 'chhattisgarhi', 'greek', 'chewa', 
#                                                 'deccan', 'akan', 'kazakh', 'northern min', 'sylheti', 'zulu', 'czech', 'kinyarwanda', 'dhundhari', 'haitian creole', 'eastern min', 'ilocano', 'quechua', 'kirundi', 'swedish', 
#                                                 'hmong', 'shona', 'uyghur', 'hiligaynon', 'mossi', 'xhosa', 'belarusian', 'balochi', 'konkani']]
#         phrase_matcher.add("LANGUAGES",None,*languages)
#         doc = self.trans_nlp(self.get_text())
#         matches = phrase_matcher(doc)
#         langs = []
#         for match_id, start, end in matches:
#             langs.append(doc[start:end].text)
#         if len(langs)>0:
#             return langs
#         else:
#             return None
# 
# 
#     def extract_name(self):
#         matcher = Matcher(self.trans_nlp.vocab)
#         nlp_text = self.trans_nlp(self.text)
#         pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]
#         matcher.add('NAME',[pattern])
#         matches = matcher(nlp_text)
#         name=[]
#         for match_id, start, end in matches:
#             span = nlp_text[start:end]
#             name.append(span.text)
#         if len(name)>0:
#             return name[0]
#         else:
#             return None
# 
# 
#     def extract_links(self):
#         if len(extract_text(self.file))>0:
#             text=extract_text(self.file)
#         else:
#             text=self.text
#         doc = self.trans_nlp(text)
#         list_of_URLs=[]
#         list_of_email=[]
#         for entity in doc:
#             if entity.like_url:
#                 list_of_URLs.append(str(entity))
#             if entity.like_email:
#                 list_of_email.append(str(entity))
#         dictionary_of_URLs={}
#         otherurllist=[]
#         for i in list_of_URLs:
#             if('linkedin' in i):
#                 if 'linkedin' in dictionary_of_URLs:
#                     dictionary_of_URLs['linkedin'].append(i)
#                 else:
#                     dictionary_of_URLs['linkedin']=[i]
#             elif('github' in i):
#                 if 'github' in dictionary_of_URLs:
#                     dictionary_of_URLs['github'].append(i)
#                 else:
#                     dictionary_of_URLs['github']=[i]
#             elif('gitlab' in i):
#                 if 'gitlab' in dictionary_of_URLs:
#                     dictionary_of_URLs['gitlab'].append(i)
#                 else:
#                     dictionary_of_URLs['gitlab']=[i]
#             elif('medium' in i):
#                 if 'medium' in dictionary_of_URLs:
#                     dictionary_of_URLs['medium'].append(i)
#                 else:
#                     dictionary_of_URLs['medium']=[i]
#             elif('dribble' in i):
#                 if 'dribble' in dictionary_of_URLs:
#                     dictionary_of_URLs['dribble'].append(i)
#                 else:
#                     dictionary_of_URLs['dribble']=[i]
#             elif('wordpress' in i):
#                 if 'wordpress' in dictionary_of_URLs:
#                     dictionary_of_URLs['wordpress'].append(i)
#                 else:
#                     dictionary_of_URLs['wordpress']=[i]
#             elif('kaggle' in i):
#                 if 'kaggle' in dictionary_of_URLs:
#                     dictionary_of_URLs['kaggle'].append(i)
#                 else:
#                     dictionary_of_URLs['kaggle']=[i]
#             else:
#                 otherurllist.append(i)
#         dictionary_of_URLs['other_urls']=otherurllist
#         dictionary_of_URLs['emails']=list_of_email
#         return dictionary_of_URLs
#     
# 
#     def get_number_of_months_from_dates(self,date1, date2):
#         if date2.lower() == 'present':
#             date2 = datetime.now().strftime('%b %Y')
#         try:
#             if len(date1.split()[0]) > 3:
#                 date1 = date1.split()
#                 date1 = date1[0][:3] + ' ' + date1[1]
#             if len(date2.split()[0]) > 3:
#                 date2 = date2.split()
#                 date2 = date2[0][:3] + ' ' + date2[1]
#         except IndexError:
#             return 0
#         try:
#             date1 = datetime.strptime(str(date1), '%b %Y')
#             date2 = datetime.strptime(str(date2), '%b %Y')
#             months_of_experience = relativedelta.relativedelta(date2, date1)
#             months_of_experience = (months_of_experience.years* 12 + months_of_experience.months)
#         except ValueError:
#             return 0
#         return months_of_experience
# 
# 
#     def get_total_experience(self):
#         if len(extract_text(self.file))>0:
#             text=extract_text(self.file)
#         else:
#             text=self.text
#         doctext = self.trans_nlp(text)
#         t=[i for i in str(doctext).split('\n')]
#         exp_ = []
#         experience_list=t
#         for line in experience_list:
#             experience = re.search(
#                 r'(?P<fmonth>\w+\s\d+)\s+\D+\s+(?P<smonth>\D+\s\d+)',
#                 line,
#                 re.I
#             )
#             if experience:
#                 exp_.append(experience.groups())
#         total_exp = sum([self.get_number_of_months_from_dates(i[0], i[1]) for i in exp_])
#         total_experience_in_months = str(total_exp)+" Months"
#         return total_experience_in_months
# 
# 
#     def extract_homelocation(self):  
#         seg=" "
#         home_address = (
#             'address',
#             'home',
#             'home address'
#         )
#         line_number = 0
#         text=self.text
#         for line in text.splitlines():
#             line_number += 1 
#             if len(line)==0 or line[0].islower():
#                 continue
#             for string_to_search in home_address:
#                 if line.lower().startswith(string_to_search):
#                     break
#             else:
#                 continue
#             break
#         ln=0
#         for line in text.splitlines():
#             ln+=1
#             if ln>=line_number:
#                 seg=seg+"\n"+line
#         nlp = en_core_web_lg.load()  
#         doc=nlp(seg)
#         homeloc=''
#         for ent in doc.ents: 
#             if ent.label_ in ["GPE"]:
#                 if "+91" not in ent.text:
#                     homeloc=ent.text
#                     return homeloc
#         doc1=nlp(self.text)
#         for ent in doc1.ents:
#             if ent.label_ =='GPE' or ent.label_ =='LOC':
#                 if "+91" not in ent.text:
#                     homeloc=ent.text
#                     return homeloc
#         return None
# 
# 
#     def extract_worklocation(self):
#         seg=" "
#         work_and_employment = (
#             'career profile',
#             'employment history',
#             'work history',
#             'work experience',
#             'experience',
#             'professional experience',
#             'professional background',
#             'additional experience',
#             'career related experience',
#             'related experience',
#             'programming experience',
#             'freelance',
#             'freelance experience',
#             'army experience',
#             'military experience',
#             'military background',
#         )
#         line_number = 0
#         abc=self.text
#         for line in abc.splitlines():
#             line_number += 1
#             if len(line)==0 or line[0].islower():
#                 continue
#             
#             for string_to_search in work_and_employment:
#             
#                 if line.lower().startswith(string_to_search):
#                     break
#             else:
#                 continue
#             break
#         ln=0
#         for line in abc.splitlines():
#             ln+=1
#             if ln>=line_number:
#                 seg=seg+"\n"+line
#         nlp = en_core_web_lg.load()  
#         ruler = nlp.add_pipe("entity_ruler")
#         ruler.add_patterns([{"label": "GPE", "pattern": "Remote"}])
#         doc=nlp(seg)
#         workloc=[]
#         for ent in doc.ents:
#             if ent.label_ in ["GPE"]:
#                 workloc.append(ent.text)
#         if(len(workloc))==0:
#             return None
#         else:
#             return workloc[0]
# 
# 
#     def extract_current_employer(self):
#         segment=" "    
#         work_and_employment = (
#             'career profile',
#             'employment history',
#             'work history',
#             'work experience',
#             'experience',
#             'professional experience',
#             'professional background',
#             'additional experience',
#             'career related experience',
#             'related experience',
#             'programming experience',
#             'freelance',
#             'freelance experience',
#             'army experience',
#             'military experience',
#             'military background',
#         )
#         line_number = 0
#         Resume_text=self.get_text()
#         for line in Resume_text.splitlines():
#             line_number += 1
#             if line[0].islower():
#                 continue
#             # For each line, check if line starts with any string from the list of strings
#             for string_to_search in work_and_employment:
#                 if line.lower().startswith(string_to_search):
#                     break
#             else:
#                 continue
#             break
#         ln=0
#         for line in Resume_text.splitlines():
#             ln+=1
#             if ln>=line_number:
#                 segment=segment+"\n"+line
#         doc=self.trans_nlp(segment)
#         Company_name=[]
#         for ent in doc.ents:
#             if ent.label_ in ["ORG"]:
#                 Company_name.append(ent.text)
#         if len(Company_name)>0:
#             return Company_name[0]
#         else:
#             return None
# 
#   
#     def extract_current_designation(self):
#         segment1=" "    #Part of resume after headings for Work Experience 
#         work_and_employment = (
#             'career profile',
#             'employment history',
#             'work history',
#             'work experience',
#             'experience',
#             'professional experience',
#             'professional background',
#             'additional experience',
#             'career related experience',
#             'related experience',
#             'programming experience',
#             'freelance',
#             'freelance experience',
#             'army experience',
#             'military experience',
#             'military background',
#         )
#         line_number = 0
#         Resume_text=self.get_text()
#         for line in Resume_text.splitlines():
#             line_number += 1
#             if line[0].islower():
#                 continue
#             for string_to_search in work_and_employment:
#                 if line.lower().startswith(string_to_search):
#                     break
#             else:
#                 continue
#             break
#         ln=0
#         for line in Resume_text.splitlines():
#             ln+=1
#             if ln>=line_number:
#                 segment1=segment1+"\n"+line
#         terms= open("Designations.txt","r",encoding='utf-8')
#         nlp = spacy.load("en_core_web_sm")
#         phrase_matcher=PhraseMatcher(nlp.vocab)
#         patterns = [nlp.make_doc(text) for text in terms]
#         phrase_matcher.add("Designation",None,*patterns)
#         doc = nlp(segment1)
#         matches = phrase_matcher(doc)
#         des=[]
#         for match_id, start, end in matches:
#             des.append(doc[start:end].text)
#         if len(des)>0:
#             return des
#         else:
#             return None
# 
# 
#     def resume_parser_wrapper(self):
#         cur_des=self.extract_current_designation()
#         cur_emp=self.extract_current_employer()
#         work_loc=self.extract_worklocation()
#         home_loc=self.extract_homelocation()
#         exp=self.get_total_experience()
#         links=self.extract_links()
#         mo_nums=self.extract_mobile_number()
#         gpa=self.extract_gpa()
#         skills=self.extract_skills()
#         lang=self.extract_languages()
#         name=self.extract_name()
#         dic={
#             'name':name,
#              'current_employer':cur_emp,
#              'current_designation':cur_des,
#              'work_location':work_loc,
#              'home_location':home_loc,
#              'experience':exp,
#              'links':links,
#              'mobile_numbers':mo_nums,
#              'gpa':gpa,
#              'skills':skills,
#              'language':lang,
#         }
#         f = open("resume.json", "w")
#         json.dump(dic, f)
#         f.close()
#         
# 
# def save_uploadedfile(uploadedfile):
#      with open(os.path.join("",uploadedfile.name),"wb") as f:
#          f.write(uploadedfile.getbuffer())
#          resume_file=uploadedfile.name
#         # resume_file=uploadedfile.name
#          resume = ResumeParser(resume_file)
#          resume.resume_parser_wrapper()
# 
#      return st.write("File Uploaded Successfully Wait for Result")
# 
# with header:
#     st.title("Welcome to my Resume Parser & Builder Project!")
#     st.text("In this project we are doing parsing using nlp and Spacy Library")
# 
# with fileupload:
#     st.header("Upload Your Resume") 
#     resume=st.file_uploader("Upload Your Resume",type=["pdf","docx","txt"])
#     if st.button("Upload"):
#         if resume is not None:
#             save_uploadedfile(resume)
#             st.header("Result")
#             with open('resume.json', 'r') as outfile:
#               summaries = json.load(outfile)
#               st.write(summaries)
#               print(summaries)
# 
# 
#

!streamlit run app.py & npx localtunnel --port 8501